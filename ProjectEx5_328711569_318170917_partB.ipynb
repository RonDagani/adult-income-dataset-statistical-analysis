{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Bayesian Approach\n",
    "\n",
    "Is the one who earns more (>50K) necessarily in the different age category from the one who earns less (<=50)?\n",
    "\n",
    "X- age Y-income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecation of jupyter warnings about sns functions that will be deprecated in the future. (sns.distplot)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Reading a CSV file into pandas Dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.stats import f\n",
    "from scipy.stats import norm\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "random.seed(1)\n",
    "# Removing missing values\n",
    "missing_values = [\"n/a\", \"na\", \"--\",\"?\"]\n",
    "df_full = pd.read_csv(\"adult.csv\",sep=\",\", na_values = missing_values)\n",
    "df_full=df_full.dropna()\n",
    "\n",
    "# Converting string columns to binary.\n",
    "df_full['gender'] =df_full['gender'].map({'Female': 0, 'Male': 1})\n",
    "df_full['income'] = df_full['income'].map({'>50K': 1, '<=50K': 0})\n",
    "\n",
    "df_200=df_full.sample(n = 200)\n",
    "df_without_sub_sample=df_full.drop(df_200.index)\n",
    "df_1000=df_without_sub_sample.sample(n = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define binary indicators Z_i\n",
    "def get_indicator(x,threshold):\n",
    "    if x>threshold:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "threshold_of_viewed_data=df_200['age'].median()\n",
    "df_200['Z']=[get_indicator(x,threshold_of_viewed_data) for x in df_200['age'].values]\n",
    "\n",
    "threshold_of_past_data=df_1000['age'].median()\n",
    "df_1000['Z']=[get_indicator(x,threshold_of_past_data) for x in df_1000['age'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define:\n",
    "\n",
    "$ P(Z=1|Y=1)={p_1} $ \n",
    "\n",
    "$ P(Z=1|Y=0)={p_2} $\n",
    "\n",
    "$ \\eta(p)=log(p/1-p)$ \n",
    "\n",
    "$\\psi = \\eta(p1)-\\eta(p2) = log((x_{00}*x_{11})/(x_{01}*x_{10}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation of log OR is: 0.7693214114555615\n",
      "Confidence Interval of log OR is [0.5795282192985598, 0.9591146036125633]\n"
     ]
    }
   ],
   "source": [
    "# estimation of p1,p2\n",
    "def estimate_p(df,category):\n",
    "    indicators=df[df['income'] ==category]['Z'].values\n",
    "    total=sum(indicators)\n",
    "    p=total/len(indicators)\n",
    "    return p\n",
    "\n",
    "p1=estimate_p(df_200,1)\n",
    "p2=estimate_p(df_200,0)\n",
    "\n",
    "psi_a=math.log(p1/(1-p1))-math.log(p2/(1-p2))\n",
    "print('Estimation of log OR is: {}'.format(psi_a))\n",
    "\n",
    "# esimates stansart eror of log odds ratio using bootstrap\n",
    "def estimate_se_via_bootstrap(df,psi,B,n=150):\n",
    "    sampled_se_list=[]\n",
    "    for i in range(B):\n",
    "        df=df.sample(n, replace=True)\n",
    "        df=df.reset_index(drop=True)\n",
    "        x00=df[df['income'] ==0][df['Z'] ==0].shape[0]\n",
    "        x10=df[df['income'] ==1][df['Z'] ==0].shape[0]\n",
    "        x01=df[df['income'] ==0][df['Z'] ==1].shape[0]\n",
    "        x11=df[df['income'] ==1][df['Z'] ==1].shape[0]\n",
    "        se=1/x00 if x00!=0 else 0\n",
    "        se=se+1/x10 if x10!=0 else se\n",
    "        se=se+1/x01 if x01!=0 else se\n",
    "        se=se+1/x11 if x11!=0 else se\n",
    "        psi_se=(np.sqrt(se))*psi\n",
    "        sampled_se_list.append(psi_se)\n",
    "    return sum(sampled_se_list)/B\n",
    "\n",
    "\n",
    "se = estimate_se_via_bootstrap(df_200,psi_a,B=500)\n",
    "\n",
    "\n",
    "z_quantile=norm.ppf(1-0.05/2)\n",
    "CI=[psi_a-z_quantile*se,psi_a+z_quantile*se]\n",
    "    \n",
    "print('Confidence Interval of log OR is {}'.format(CI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "<a href=\"https://ibb.co/c1mpj5T\"><img src=\"https://i.ibb.co/c1mpj5T/2-b.png\" alt=\"2-b\" border=\"0\"></a> \n",
    "\n",
    "(if you cant see the picture, please enter the link manually, to see the calculations.)\n",
    "\n",
    "For standart uniform prior we know that MAP estimator of $p_1$ equal to MLE estimator.\n",
    "\n",
    "From tutorial 6 we know MLE for $p_1$ is $\\bar X_{11}/n$  and for $p_2$ is $\\bar X_{01}/m$ \n",
    "\n",
    "$X_{01}$ ~ $Binomial(X_{0},p_2)$\n",
    "\n",
    "$X_{11}$ ~ $Binomial(X_{1},p_1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation of log OR is: 1.4053425560905852\n",
      "Credible Interval of psi is [1.2487576215884806, 1.5634423426247914]\n"
     ]
    }
   ],
   "source": [
    "p1_b=df_200[df_200['income'] ==1][df_200['Z'] ==1]['Z'].mean()/df_200[df_200['income'] ==1].shape[0]\n",
    "p2_b=df_200[df_200['income'] ==0][df_200['Z'] ==1]['Z'].mean()/df_200[df_200['income'] ==0].shape[0]\n",
    "psi_b=math.log(p1_b/(1-p1_b))-math.log(p2_b/(1-p2_b))\n",
    "print('Estimation of log OR is: {}'.format(psi_b))\n",
    "\n",
    "def get_credible_int_b(df,sample_size,y,z):\n",
    "    a=df[df['income'] ==y][df['Z'] ==z]['Z'].sum()+1\n",
    "    b=df[df['income'] ==y].shape[0]*df[df['income'] ==y][df['Z'] ==z].shape[0]-df[df['income'] ==y][df['Z'] ==z]['Z'].sum()+1\n",
    "    samples=np.random.beta(a, b, sample_size)\n",
    "    samples.sort()\n",
    "    credible_int=[samples[int(0.05*sample_size/2)],samples[int((1-0.05/2)*sample_size)]]\n",
    "    return credible_int\n",
    "ci_p1=get_credible_int_b(df_200,100000,1,1)\n",
    "ci_p2=get_credible_int_b(df_200,100000,0,1)\n",
    "ci_psi=[math.log(ci_p1[0]/(1-ci_p1[0]))-math.log(ci_p2[0]/(1-ci_p2[0])),math.log(ci_p1[1]/(1-ci_p1[1]))-math.log(ci_p2[1]/(1-ci_p2[1]))]\n",
    "\n",
    "print(\"Credible Interval of psi is {}\".format(ci_psi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting jeffreys prior and solving MAP we get that estimator for $p_1$ is $(\\Sigma^N X_{11} +0.5)/(1+Nn)$\n",
    "and for $p_2$ is $(\\Sigma^M X_{01}+0.5)/(1+Mm)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://ibb.co/jZ7HKnr\"><img src=\"https://i.ibb.co/jZ7HKnr/2-c.png\" alt=\"2-c\" border=\"0\"></a> \n",
    "\n",
    "\n",
    "(if you cant see the picture, please enter the link manually, to see the calculations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation of log OR is: 1.4172062381878079\n",
      "Credible Interval of psi is [1.259031984782486, 1.5727201174502246]\n"
     ]
    }
   ],
   "source": [
    "p1_c=(df_200[df_200['income'] ==1][df_200['Z'] ==1]['Z'].sum()+0.5)/(1+df_200[df_200['income'] ==1].shape[0]*df_200[df_200['income'] ==1][df_200['Z'] ==1].shape[0])\n",
    "p2_c=(df_200[df_200['income'] ==0][df_200['Z'] ==1]['Z'].sum()+0.5)/(1+df_200[df_200['income'] ==0].shape[0]*df_200[df_200['income'] ==0][df_200['Z'] ==1].shape[0])\n",
    "psi_c=math.log(p1_c/(1-p1_c))-math.log(p2_c/(1-p2_c))\n",
    "print('Estimation of log OR is: {}'.format(psi_c))\n",
    "\n",
    "def get_credible_int_c(df,sample_size,y,z):\n",
    "    a=df[df['income'] ==y][df['Z'] ==z]['Z'].sum()+1.5\n",
    "    b=df[df['income'] ==y].shape[0]*df[df['income'] ==y][df['Z'] ==z].shape[0]-df[df['income'] ==y][df['Z'] ==z]['Z'].sum()+1.5\n",
    "    samples=np.random.beta(a, b, sample_size)\n",
    "    samples.sort()\n",
    "    credible_int=[samples[int(0.05*sample_size/2)],samples[int((1-0.05/2)*sample_size)]]\n",
    "    return credible_int\n",
    "ci_p1=get_credible_int_c(df_200,10000,1,1)\n",
    "ci_p2=get_credible_int_c(df_200,10000,0,1)\n",
    "ci_psi=[math.log(ci_p1[0]/(1-ci_p1[0]))-math.log(ci_p2[0]/(1-ci_p2[0])),math.log(ci_p1[1]/(1-ci_p1[1]))-math.log(ci_p2[1]/(1-ci_p2[1]))]\n",
    "\n",
    "print(\"Credible Interval of psi is {}\".format(ci_psi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d.\n",
    "\n",
    "<a href=\"https://ibb.co/qFfXndw\"><img src=\"https://i.ibb.co/qFfXndw/2-d.png\" alt=\"2-d\" border=\"0\"></a>\n",
    "\n",
    "(if you cant see the picture, please enter the link manually, to see the calculations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation of log OR is: 1.4291148347524092\n",
      "Credible Interval of psi is [1.2511512653577856, 1.5626026699806865]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "def get_betas_params(observations):\n",
    "    a, b, loc, scale =stats.beta.fit(observations)\n",
    "    return  (a,b)\n",
    "\n",
    "a_p1,b_p1=get_betas_params(df_1000[df_1000['income'] ==1][df_1000['Z'] ==1]['Z'].values.tolist())\n",
    "a_p2,b_p2=get_betas_params(df_1000[df_1000['income'] ==0][df_1000['Z'] ==1]['Z'].values.tolist())\n",
    "\n",
    "\n",
    "p1_d=(df_200[df_200['income'] ==1][df_200['Z'] ==1]['Z'].sum()+a_p1)/(a_p1+b_p1+df_200[df_200['income'] ==1].shape[0]*df_200[df_200['income'] ==1][df_200['Z'] ==1].shape[0])\n",
    "p2_d=(df_200[df_200['income'] ==0][df_200['Z'] ==1]['Z'].sum()+a_p2)/(a_p2+b_p2+df_200[df_200['income'] ==0].shape[0]*df_200[df_200['income'] ==0][df_200['Z'] ==1].shape[0])\n",
    "psi_d=math.log(p1_d/(1-p1_d))-math.log(p2_d/(1-p2_d))\n",
    "print('Estimation of log OR is: {}'.format(psi_d))\n",
    "\n",
    "def get_credible_int_d(df,sample_size,y,z,alpha,beta):\n",
    "    a=df[df['income'] ==y][df['Z'] ==z]['Z'].sum()+alpha\n",
    "    b=df[df['income'] ==y].shape[0]*df[df['income'] ==y][df['Z'] ==z].shape[0]-df[df['income'] ==y][df['Z'] ==z]['Z'].sum()+beta\n",
    "    samples=np.random.beta(a, b, sample_size)\n",
    "    samples.sort()\n",
    "    credible_int=[samples[int(0.05*sample_size/2)],samples[int((1-0.05/2)*sample_size)]]\n",
    "    return credible_int\n",
    "\n",
    "ci_p1=get_credible_int_b(df_200,10000,1,1)\n",
    "ci_p2=get_credible_int_b(df_200,10000,0,1)\n",
    "ci_psi=[math.log(ci_p1[0]/(1-ci_p1[0]))-math.log(ci_p2[0]/(1-ci_p2[0])),math.log(ci_p1[1]/(1-ci_p1[1]))-math.log(ci_p2[1]/(1-ci_p2[1]))]\n",
    "\n",
    "print(\"Credible Interval of psi is {}\".format(ci_psi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### e.\n",
    "All the estomators are quite simmilar.But, estimator from 2.a is less similar to the others.\n",
    "We tend to think that estimator from 2.d is more accurate than the others.\n",
    "Jaffrey's prior is better than the flat one and it is non informative.\n",
    "Prior calculated via past samples, assumes knowlege about the disrtibution and seems to be more precise, because in retrospect all the data came from the same data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.a Estimation of log OR is: 0.7693214114555615\n",
      "2.b Estimation of log OR is: 1.4053425560905852\n",
      "2.c Estimation of log OR is: 1.4172062381878079\n",
      "2.d Estimation of log OR is: 1.4291148347524092\n"
     ]
    }
   ],
   "source": [
    "print('2.a Estimation of log OR is: {}'.format(psi_a))\n",
    "print('2.b Estimation of log OR is: {}'.format(psi_b))\n",
    "print('2.c Estimation of log OR is: {}'.format(psi_c))\n",
    "print('2.d Estimation of log OR is: {}'.format(psi_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are age, gender and education (edication_num) correlated with hours per week?\n",
    "Y=hours per week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000=df_full.sample(n = 1000)\n",
    "df_1000['aux_columns_of_ones']= [1]*df_1000.shape[0]\n",
    "df_1000 = df_1000[['aux_columns_of_ones','gender','age','educational-num','hours-per-week']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, df_full does not consist any missing values, therefore, we'll get the required data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CI per beta:\n",
      "beta0 is: 26.888053434943288, ci is: [23.239600629374543, 30.536506240512033]\n",
      "beta1 is: 5.596043421944073, ci is: [4.130282598711968, 7.061804245176178]\n",
      "beta2 is: 0.09007758503883385, ci is: [0.034287266220053375, 0.14586790385761433]\n",
      "beta3 is: 0.6748538092353172, ci is: [0.3973435827563493, 0.9523640357142852]\n"
     ]
    }
   ],
   "source": [
    "def create_targets_and_lables(df_data):\n",
    "    X=df_data.iloc[:, 0:df_data.shape[1]-1].values\n",
    "    Y=df_data.iloc[:, df_data.shape[1]-1:].values\n",
    "    return X,Y\n",
    "\n",
    "def estimate_beta(df_data):\n",
    "    X,Y =create_targets_and_lables(df_data)\n",
    "    beta_est=((np.linalg.pinv(X.T@X))@X.T)@Y\n",
    "    return beta_est\n",
    "  \n",
    "\n",
    "X_sampled,Y_sampled=create_targets_and_lables(df_1000)\n",
    "beta_est=estimate_beta(df_1000)\n",
    "\n",
    "Y_pred = X_sampled@beta_est\n",
    "\n",
    "n_p=X_sampled.shape[0]-X_sampled.shape[1]\n",
    "mse_resid=np.sum(np.square(Y_sampled-Y_pred))/n_p\n",
    "C = np.linalg.inv(X_sampled.T@X_sampled)\n",
    "C *= mse_resid\n",
    "SE = np.sqrt(C)\n",
    "\n",
    "\n",
    "CI_on_cov_matrix={}\n",
    "z_quantile=norm.ppf(1-0.05/2)\n",
    "print('CI per beta:')\n",
    "\n",
    "for i,beta in enumerate(beta_est):\n",
    "    ci=[beta[0]-z_quantile*SE[i][i],beta[0]+z_quantile*SE[i][i]]\n",
    "    CI_on_cov_matrix[\"beta_{}\".format(i)]=ci\n",
    "    print(\"beta{} is: {}, ci is: {}\".format(i,beta[0],ci))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aux_columns_of_ones</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>random_ber_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9846</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46684</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45507</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42061</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29205</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>90</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44644</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>90</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22351</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10772</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aux_columns_of_ones  gender   age  educational-num hours-per-week  \\\n",
       "9846                   1.0     0.0  24.0             12.0              3   \n",
       "46684                  1.0     0.0  56.0              9.0              4   \n",
       "45507                  1.0     0.0  17.0              6.0              5   \n",
       "42061                  1.0     0.0  20.0             10.0              6   \n",
       "15795                  1.0     1.0  59.0             14.0              7   \n",
       "...                    ...     ...   ...              ...            ...   \n",
       "25999                  1.0     1.0  49.0             15.0            NaN   \n",
       "29205                  1.0     1.0  37.0             13.0             90   \n",
       "44644                  1.0     1.0  52.0             10.0             90   \n",
       "22351                  1.0     1.0  48.0              9.0            NaN   \n",
       "10772                  1.0     1.0  51.0             10.0             99   \n",
       "\n",
       "       random_ber_var  \n",
       "9846              0.0  \n",
       "46684             0.0  \n",
       "45507             0.0  \n",
       "42061             0.0  \n",
       "15795             0.0  \n",
       "...               ...  \n",
       "25999             1.0  \n",
       "29205             1.0  \n",
       "44644             1.0  \n",
       "22351             1.0  \n",
       "10772             1.0  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1000=df_1000.sort_values(by=['hours-per-week'])\n",
    "\n",
    "random_ber_vars=[]\n",
    "for i,sample in enumerate(df_1000['hours-per-week'].values):\n",
    "    random_ber_vars.append(np.random.binomial(1, i/(i+2)))\n",
    "    \n",
    "df_1000['random_ber_var']= random_ber_vars  \n",
    "# sample 500 samples with bernoulli == 1 and remove them.\n",
    "df=df_1000.query(\"random_ber_var == 1\").sample(n=500)\n",
    "df.loc[df['random_ber_var'] == 1, 'hours-per-week'] = 'NaN'\n",
    "df_1000.update(df)\n",
    "df_1000.loc[df_1000['hours-per-week'] == 'NaN', 'hours-per-week'] = np.nan\n",
    "df_1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CI per beta:\n",
      "beta0 is: 27.416655414288144, ci is: [22.035312495952894, 32.79799833262339], beta0 is in the ci.\n",
      "beta1 is: 5.957272097087067, ci is: [3.8144928098215574, 8.100051384352577], beta1 is in the ci.\n",
      "beta2 is: 0.12108698838470636, ci is: [0.03830507228676118, 0.20386890448265155], beta2 is in the ci.\n",
      "beta3 is: 0.41224902825452064, ci is: [0.012353397010886558, 0.8121446594981547], beta3 is in the ci.\n"
     ]
    }
   ],
   "source": [
    "df_1000 = df_1000[['aux_columns_of_ones','gender','age','educational-num','hours-per-week']]\n",
    "df_no_nan=df_1000.dropna()\n",
    "df_no_nan=df_no_nan[['aux_columns_of_ones','gender','age','educational-num','hours-per-week']]\n",
    "X_sampled,Y_sampled=create_targets_and_lables(df_no_nan)\n",
    "beta_est=estimate_beta(df_no_nan)\n",
    "\n",
    "Y_pred = X_sampled@beta_est\n",
    "\n",
    "n_p=X_sampled.shape[0]-X_sampled.shape[1]\n",
    "mse_resid=np.sum(np.square(Y_sampled-Y_pred))/n_p\n",
    "C = np.linalg.inv(X_sampled.T@X_sampled)\n",
    "C *= mse_resid\n",
    "SE = np.sqrt(C)\n",
    "\n",
    "\n",
    "CI_on_cov_matrix={}\n",
    "z_quantile=norm.ppf(1-0.05/2)\n",
    "print('CI per beta:')\n",
    "\n",
    "for i,beta in enumerate(beta_est):\n",
    "    ci=[beta[0]-z_quantile*SE[i][i],beta[0]+z_quantile*SE[i][i]]\n",
    "    CI_on_cov_matrix[\"beta_{}\".format(i)]=ci\n",
    "    if beta[0]< ci[1] and beta[0]> ci[0]:\n",
    "        temp = 'in'\n",
    "    else:\n",
    "        temp ='not in'\n",
    "    print(\"beta{} is: {}, ci is: {}, beta{} is {} the ci.\".format(i,beta[0],ci,i, temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CI per beta:\n",
      "beta0 is: 27.416655414286602, ci is: [27.41665541428645, 27.416655414286755], beta0 is in the ci.\n",
      "beta1 is: 5.957272097087174, ci is: [5.957272097087113, 5.957272097087236], beta1 is in the ci.\n",
      "beta2 is: 0.12108698838472734, ci is: [0.12108698838472505, 0.12108698838472963], beta2 is in the ci.\n",
      "beta3 is: 0.41224902825457865, ci is: [0.4122490282545669, 0.4122490282545904], beta3 is in the ci.\n"
     ]
    }
   ],
   "source": [
    "# data wihout missing values. will be used in regression imputation\n",
    "df_no_nan=df_1000.dropna()\n",
    "df_no_nan=df_no_nan[['aux_columns_of_ones','gender','age','educational-num','hours-per-week']]\n",
    "X_sampled,Y_sampled=create_targets_and_lables(df_no_nan)\n",
    "beta_est=estimate_beta(df_no_nan)\n",
    "\n",
    "# data with missing values.\n",
    "df_with_nan=df_1000[df_1000.isnull().any(axis=1)]\n",
    "df_with_nan=df_with_nan[['aux_columns_of_ones','gender','age','educational-num','hours-per-week']]\n",
    "X_sampled_nan, Y_sampled_nan=create_targets_and_lables(df_with_nan)\n",
    "\n",
    "# replace missing values using imputation\n",
    "Y_imputation = X_sampled_nan@beta_est\n",
    "df_b=df_with_nan.copy()\n",
    "df_b['hours-per-week']=Y_imputation\n",
    "\n",
    "# estimate beta using new data.\n",
    "beta_est=estimate_beta(df_b)\n",
    "Y_pred = X_sampled_nan@beta_est\n",
    "\n",
    "n_p=X_sampled_nan.shape[0]-X_sampled_nan.shape[1]\n",
    "mse_resid=np.sum(np.square(Y_imputation-Y_pred))/n_p\n",
    "C = np.linalg.inv(X_sampled_nan.T@X_sampled_nan)\n",
    "C *= mse_resid\n",
    "SE = np.sqrt(C)\n",
    "\n",
    "\n",
    "CI_on_cov_matrix={}\n",
    "z_quantile=norm.ppf(1-0.05/2)\n",
    "print('CI per beta:')\n",
    "\n",
    "\n",
    "for i,beta in enumerate(beta_est):\n",
    "    ci=[beta[0]-z_quantile*SE[i][i],beta[0]+z_quantile*SE[i][i]]\n",
    "    CI_on_cov_matrix[\"beta_{}\".format(i)]=ci\n",
    "    if beta[0]< ci[1] and beta[0]> ci[0]:\n",
    "        temp = 'in'\n",
    "    else:\n",
    "        temp ='not in'\n",
    "    print(\"beta{} is: {}, ci is: {}, beta{} is {} the ci.\".format(i,beta[0],ci,i, temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we got the same results as we got in section a - all betas are in their ci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CI per beta:\n",
      "beta0 is: 27.416655414283603, ci is: [27.416655414283202, 27.416655414284005], beta0 is in the ci.\n",
      "beta1 is: 5.957272097087537, ci is: [5.957272097087373, 5.9572720970877], beta1 is in the ci.\n",
      "beta2 is: 0.12108698838474666, ci is: [0.12108698838474054, 0.12108698838475278], beta2 is in the ci.\n",
      "beta3 is: 0.4122490282547587, ci is: [0.41224902825472726, 0.4122490282547901], beta3 is in the ci.\n"
     ]
    }
   ],
   "source": [
    "# data wihout missing values. will be used in regression imputation\n",
    "df_no_nan=df_1000.dropna()\n",
    "df_no_nan=df_no_nan[['aux_columns_of_ones','gender','age','educational-num','hours-per-week']]\n",
    "X_sampled,Y_sampled=create_targets_and_lables(df_no_nan)\n",
    "beta_est=estimate_beta(df_no_nan)\n",
    "\n",
    "# data with missing values.\n",
    "df_with_nan=df_1000[df_1000.isnull().any(axis=1)]\n",
    "df_with_nan=df_with_nan[['aux_columns_of_ones','gender','age','educational-num','hours-per-week']]\n",
    "X_sampled_nan, Y_sampled_nan=create_targets_and_lables(df_with_nan)\n",
    "\n",
    "\n",
    "def multiple_imputations(m,df,df_no_nan,df_with_nan):\n",
    "    imputations=[]\n",
    "    X_sampled,_=create_targets_and_lables(df)\n",
    "    mu, sigma = norm.fit(X_sampled)\n",
    "    X_sampled_nan, _=create_targets_and_lables(df_with_nan)\n",
    "\n",
    "    betas_estimations=[]\n",
    "    for i in range(0,m):\n",
    "        # replace missing values using imputation for each i- Step 1\n",
    "        Y_imputation_i = np.random.normal(mu, sigma, 500)\n",
    "        df_i=df_with_nan.copy()\n",
    "        df_i['hours-per-week']=Y_imputation\n",
    "        df_i=pd.concat([df_i,df_no_nan])\n",
    "        # estimate beta using new data for each i- Step 2\n",
    "        beta_est=estimate_beta(df_i)\n",
    "        betas_estimations.append(beta_est)\n",
    "        Y_pred = X_sampled_nan@beta_est\n",
    "        imputations.append(Y_pred)\n",
    "#         return avg. of imputations,avg. of betas (estamotion of betas-Step 3), betas \n",
    "    return (np.mean(imputations, axis=0),np.mean(betas_estimations, axis=0),betas_estimations)\n",
    "\n",
    "\n",
    "Y_pred,betas,betas_estimations=multiple_imputations(100,df_1000,df_no_nan,df_with_nan)\n",
    "\n",
    "n_p=X_sampled_nan.shape[0]-X_sampled_nan.shape[1]\n",
    "mse_resid=np.sum(np.square(Y_imputation-Y_pred))/n_p\n",
    "C = np.linalg.inv(X_sampled_nan.T@X_sampled_nan)\n",
    "C *= mse_resid\n",
    "SE = np.sqrt(C)\n",
    "\n",
    "\n",
    "CI_on_cov_matrix={}\n",
    "z_quantile=norm.ppf(1-0.05/2)\n",
    "print('CI per beta:')\n",
    "\n",
    "for i,beta in enumerate(betas):\n",
    "    ci=[beta[0]-z_quantile*SE[i][i],beta[0]+z_quantile*SE[i][i]]\n",
    "    CI_on_cov_matrix[\"beta_{}\".format(i)]=ci\n",
    "    if beta[0]< ci[1] and beta[0]> ci[0]:\n",
    "        temp = 'in'\n",
    "    else:\n",
    "        temp ='not in'\n",
    "    print(\"beta{} is: {}, ci is: {}, beta{} is {} the ci.\".format(i,beta[0],ci,i, temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation of s.e of each beta using Rubun's formula: [1.11794573 1.11794573 1.11794573 1.11794573]\n",
      "\n",
      "beta0 is: 27.416655414283603, ci is: [27.226862222126602, 27.606448606440605], beta0 is in the ci.\n",
      "beta1 is: 5.957272097087537, ci is: [5.767478904930535, 6.147065289244539], beta1 is in the ci.\n",
      "beta2 is: 0.12108698838474666, ci is: [-0.06870620377225509, 0.3108801805417484], beta2 is in the ci.\n",
      "beta3 is: 0.4122490282547587, ci is: [0.22245583609775693, 0.6020422204117604], beta3 is in the ci.\n"
     ]
    }
   ],
   "source": [
    "def within_imputation_variance(imputations):\n",
    "    summations=0\n",
    "    M=len(imputations)\n",
    "    for imputation in imputations:\n",
    "        summation=+imputation.var()\n",
    "    \n",
    "    return summation/M\n",
    "\n",
    "\n",
    "def between_imputation_variance(Y_pred,imputations):\n",
    "    summations=[]*len(beta_est)\n",
    "    M=len(imputations)\n",
    "    for imputation in imputations:\n",
    "        diff=imputation-Y_pred\n",
    "        summations=+(imputation-Y_pred)**2\n",
    "    flat_list =np.asarray([val for sublist in summations for val in sublist])\n",
    "    return (M+1)*flat_list/(M*(M-1))\n",
    "\n",
    "se_est=np.sqrt(within_imputation_variance(betas_estimations)+between_imputation_variance(betas,betas_estimations))\n",
    "print(\"Estimation of s.e of each beta using Rubun's formula: {}\".format(se_est))\n",
    "print()\n",
    "\n",
    "z_quantile=norm.ppf(1-0.05/2)\n",
    "for i,beta in enumerate(betas):\n",
    "    ci=[beta[0]-z_quantile*se,beta[0]+z_quantile*se]\n",
    "    CI_on_cov_matrix[\"beta_{}\".format(i)]=ci\n",
    "    if beta[0]< ci[1] and beta[0]> ci[0]:\n",
    "        temp = 'in'\n",
    "    else:\n",
    "        temp ='not in'\n",
    "    print(\"beta{} is: {}, ci is: {}, beta{} is {} the ci.\".format(i,beta[0],ci,i, temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e.\n",
    "As we know:\n",
    "${R|X_1,⋯, X_k} ∼ Ber(e^{X^T\\beta}/(1 + e^{X^T\\beta})) $\n",
    "\n",
    "Using logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "one = np.ones(df_no_nan.shape[0])\n",
    "X_sampled,Y_sampled=create_targets_and_lables(df_no_nan)\n",
    "log_reg = LogisticRegression(random_state=0)\n",
    "log_reg.fit(X_sampled, Y_sampled.astype('int'))\n",
    "beta_log_hat = log_reg.coef_\n",
    "\n",
    "beta_log_hat=np.mean(beta_log_hat, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f.\n",
    "\n",
    "In class we seen that IPW estimator is: $ 1/n {\\Sigma}^{n}_{i=1}R_i/\\hat{\\pi}(W_i)\\cdot s(Y_i;\\theta) $\n",
    "We'll replace s(Y_i;\\theta) with $ ||{X^T \\beta - y}|| $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pi(X,beta):\n",
    "    pi=np.exp(X@beta)/(1+np.exp(X@beta))\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "No loop matching the specified signature and casting was found for ufunc lstsq_n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-8eac371d552c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpi_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpi_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_sampled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mbeta_ipw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstsq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpi_w\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mX_sampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi_w\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mY_sampled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# estimated_betas.append(beta_ipw[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mbeta_ipw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mlstsq\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36mlstsq\u001b[1;34m(a, b, rcond)\u001b[0m\n\u001b[0;32m   2304\u001b[0m         \u001b[1;31m# lapack can't handle n_rhs = 0 - so allocate the array one larger in that axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2305\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_rhs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2306\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2307\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2308\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: No loop matching the specified signature and casting was found for ufunc lstsq_n"
     ]
    }
   ],
   "source": [
    "X_sampled,Y_sampled=create_targets_and_lables(df_no_nan)\n",
    "pi_w = calculate_pi(X_sampled,beta_log_hat)\n",
    "pi_w = np.diag(pi_w)\n",
    "print(Y_sampled.shape)\n",
    "beta_ipw = np.linalg.lstsq(pi_w@X_sampled, pi_w@Y_sampled)\n",
    "beta_ipw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
